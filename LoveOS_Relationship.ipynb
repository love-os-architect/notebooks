# Love-OS Relationship Notebook (Dynamics Simulation)

This notebook visualizes the time-series dynamics of Attraction $A(t)$, Safety $S(t)$, Clarity $C(t)$, Latency $L(t)$, and Resistance $R(t)$ in relationships using the Love-OS Information Thermodynamics model.

**Attraction Equation (Proposed)**

$$A = (M^\alpha S^\beta) \cdot \frac{C^\gamma}{1+\rho R_{avg}} \cdot \frac{1}{1+\theta L_{avg}}$$

- $R_{avg}=(R_i+R_j)/2$, $L_{avg}=(L_i+L_j)/2$
- $\alpha,\beta,\gamma,\rho,\theta > 0$: Sensitivity coefficients

import math
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from pathlib import Path

# Plotting setup
matplotlib.rcParams['figure.dpi'] = 130
matplotlib.rcParams['font.family'] = 'DejaVu Sans' # Standard font for English
plt.rcParams['axes.unicode_minus'] = False

out_dir = Path('figs_rel')
out_dir.mkdir(exist_ok=True)
print('Environment setup OK')

from dataclasses import dataclass

@dataclass
class Params:
    # Sensitivity Coefficients
    alpha: float = 1.2   # Impact of Match (Values)
    beta: float = 1.1    # Impact of Safety
    gamma: float = 1.0   # Impact of Clarity
    rho: float = 1.0     # Resistance Penalty (Ego)
    theta: float = 0.8   # Latency Penalty
    
    # Dynamics Coefficients
    k_s_up: float = 0.06      # Safety increase rate (Honesty)
    k_s_down: float = 0.04    # Safety decrease rate (Ambiguity)
    k_l_s: float = 0.05       # Latency reduction via Safety
    k_l_r: float = 0.06       # Latency increase via Resistance
    k_r_stress: float = 0.05  # Resistance increase via Stress
    k_r_align: float = 0.05   # Resistance decrease via Alignment
    k_r_practice: float = 0.05# Resistance decrease via Practice
    k_c_up: float = 0.06      # Clarity increase
    k_c_down: float = 0.04    # Clarity decrease (Confusion)

@dataclass
class State:
    R_i: float  # Resistance (User i)
    R_j: float  # Resistance (User j)
    S: float    # Shared Safety
    C: float    # Shared Clarity
    L_i: float  # Latency (User i)
    L_j: float  # Latency (User j)
    M: float    # Match (Value Alignment)

clip01 = lambda x: max(0.0, min(1.0, x))

def attraction(st: State, p: Params):
    """Calculates the gravitational force (Attraction) between two entities."""
    R_avg = (st.R_i + st.R_j) / 2.0
    L_avg = (st.L_i + st.L_j) / 2.0
    
    # Core Drivers: Match & Safety
    core = (st.M**p.alpha) * (st.S**p.beta)
    
    # Information Throughput Factors: Clarity vs Resistance
    clarity_factor = (st.C**p.gamma) / (1.0 + p.rho * R_avg)
    
    # Latency Penalty
    latency_penalty = 1.0 / (1.0 + p.theta * L_avg)
    
    return clip01(core * clarity_factor * latency_penalty)

def step(st: State, actions, p: Params, noise_scale=0.01):
    """
    Updates the state based on actions.
    actions: dict with values 0..1 for honesty, ambiguity, stress, align, practice, etc.
    """
    # 1. Update Safety (S)
    # Honesty increases safety, Ambiguity decreases it
    s_new = st.S + p.k_s_up * (actions.get('honesty_i',0) + actions.get('honesty_j',0))/2.0 \
                 - p.k_s_down * (actions.get('ambiguity_i',0) + actions.get('ambiguity_j',0))/2.0
    st.S = clip01(s_new)

    # 2. Update Latency (L)
    # Safety reduces latency, Resistance increases it
    st.L_i = clip01(st.L_i - p.k_l_s * st.S + p.k_l_r * st.R_i)
    st.L_j = clip01(st.L_j - p.k_l_s * st.S + p.k_l_r * st.R_j)

    # 3. Update Resistance (R)
    # Stress increases R; Alignment and Practice reduce R
    n_i = noise_scale * (np.random.rand() - 0.5)
    n_j = noise_scale * (np.random.rand() - 0.5)
    
    r_i = st.R_i + p.k_r_stress * actions.get('stress_i',0) \
                 - p.k_r_align  * actions.get('align_i',0) \
                 - p.k_r_practice * actions.get('practice_i',0) + n_i
                 
    r_j = st.R_j + p.k_r_stress * actions.get('stress_j',0) \
                 - p.k_r_align  * actions.get('align_j',0) \
                 - p.k_r_practice * actions.get('practice_j',0) + n_j
                 
    st.R_i = clip01(r_i)
    st.R_j = clip01(r_j)

    # 4. Update Clarity (C)
    c_new = st.C + p.k_c_up * actions.get('clarity_up',0) - p.k_c_down * actions.get('confuse_up',0)
    st.C = clip01(c_new)

    return st

def simulate(T, init: State, p: Params, policy_fn):
    traj = {k: [] for k in ['A','R_i','R_j','S','C','L_i','L_j','M']}
    st = State(**init.__dict__)
    
    for t in range(T):
        act = policy_fn(t, st)
        st = step(st, act, p)
        A_t = attraction(st, p)
        
        # Log trajectory
        for k,v in [('A',A_t),('R_i',st.R_i),('R_j',st.R_j),('S',st.S),('C',st.C),('L_i',st.L_i),('L_j',st.L_j),('M',st.M)]:
            traj[k].append(v)
            
    return traj

# Policy A: Honest, Clear, Low Latency (Love-OS Ideal)
def policy_A(t, st):
    return {
        'honesty_i': 0.9, 'ambiguity_i': 0.1,
        'honesty_j': 0.9, 'ambiguity_j': 0.1,
        'stress_i': 0.2, 'align_i': 0.7, 'practice_i': 0.7,
        'stress_j': 0.2, 'align_j': 0.7, 'practice_j': 0.7,
        'clarity_up': 0.8, 'confuse_up': 0.1
    }

# Policy B: Ambiguous, Defensive, High Latency (Ego-Driven)
def policy_B(t, st):
    return {
        'honesty_i': 0.3, 'ambiguity_i': 0.7,
        'honesty_j': 0.3, 'ambiguity_j': 0.7,
        'stress_i': 0.7, 'align_i': 0.2, 'practice_i': 0.2,
        'stress_j': 0.7, 'align_j': 0.2, 'practice_j': 0.2,
        'clarity_up': 0.2, 'confuse_up': 0.7
    }

# Policy C: One-Sided Improvement (Gradual Awakening)
def policy_C(t, st):
    prog = min(1.0, t/50.0)
    return {
        'honesty_i': 0.8, 'ambiguity_i': 0.2,
        # User J improves over time
        'honesty_j': 0.3 + 0.6*prog, 'ambiguity_j': 0.7 - 0.6*prog,
        'stress_i': 0.3, 'align_i': 0.6, 'practice_i': 0.6,
        'stress_j': 0.7 - 0.5*prog, 'align_j': 0.2 + 0.5*prog, 'practice_j': 0.2 + 0.5*prog,
        'clarity_up': 0.3 + 0.5*prog, 'confuse_up': 0.6 - 0.5*prog
    }

# Run Simulations
init = State(R_i=0.6, R_j=0.6, S=0.5, C=0.5, L_i=0.5, L_j=0.5, M=0.6)
p = Params()
T = 120

traj_A = simulate(T, init, p, policy_A)
traj_B = simulate(T, init, p, policy_B)
traj_C = simulate(T, init, p, policy_C)

# --- Visualization ---

# Plot 1: Attraction A(t)
fig, ax = plt.subplots(1, 1, figsize=(8, 3.5))
ax.plot(traj_A['A'], label='Policy A: Honest/Clear/Fast', color='C0')
ax.plot(traj_B['A'], label='Policy B: Ambiguous/Defensive/Slow', color='C1')
ax.plot(traj_C['A'], label='Policy C: One-Sided Improvement', color='C2')

ax.set_title('Comparison of Relationship Attraction A(t)')
ax.set_xlabel('Time Steps')
ax.set_ylabel('Attraction A (0â€“1)')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(out_dir / 'rel_A_time.png', bbox_inches='tight')
print('Saved: rel_A_time.png')

# Plot 2: Internal States (Scenario C)
fig, axes = plt.subplots(2, 2, figsize=(10, 6))

# Resistance R
axes[0][0].plot(traj_C['R_i'], label='R_i (User i)', color='C3')
axes[0][0].plot(traj_C['R_j'], label='R_j (User j)', color='C4')
axes[0][0].set_title('Evolution of Resistance R')
axes[0][0].legend()
axes[0][0].grid(True, alpha=0.3)

# Safety S
axes[0][1].plot(traj_C['S'], color='C0')
axes[0][1].set_title('Evolution of Safety S')
axes[0][1].grid(True, alpha=0.3)

# Clarity C
axes[1][0].plot(traj_C['C'], color='C1')
axes[1][0].set_title('Evolution of Clarity C')
axes[1][0].grid(True, alpha=0.3)

# Latency L
axes[1][1].plot(traj_C['L_i'], label='L_i', color='C2')
axes[1][1].plot(traj_C['L_j'], label='L_j', color='C5')
axes[1][1].set_title('Evolution of Latency L')
axes[1][1].legend()
axes[1][1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(out_dir / 'rel_states_time.png', bbox_inches='tight')
print('Saved: rel_states_time.png')

print(f"Policy A (Final A): {traj_A['A'][-1]:.4f}")
print(f"Policy B (Final A): {traj_B['A'][-1]:.4f}")
print(f"Policy C (Final A): {traj_C['A'][-1]:.4f}")

# Sensitivity Analysis: How rho (Ego) and theta (Latency) affect final Attraction
def final_A_with(rho, theta):
    p2 = Params(rho=rho, theta=theta)
    tr = simulate(120, State(R_i=0.6, R_j=0.6, S=0.5, C=0.5, L_i=0.5, L_j=0.5, M=0.6), p2, policy_C)
    return tr['A'][-1]

rho_list = np.linspace(0.5, 2.0, 7)
theta_list = np.linspace(0.4, 1.5, 7)
Z = np.zeros((len(rho_list), len(theta_list)))

for i, r in enumerate(rho_list):
    for j, th in enumerate(theta_list):
        Z[i, j] = final_A_with(r, th)

fig, ax = plt.subplots(1, 1, figsize=(6, 4))
im = ax.imshow(Z, origin='lower', aspect='auto', cmap='viridis', 
               extent=[theta_list[0], theta_list[-1], rho_list[0], rho_list[-1]])

ax.set_xlabel('Theta (Latency Sensitivity)')
ax.set_ylabel('Rho (Resistance Sensitivity)')
ax.set_title('Sensitivity Map of Final Attraction (A)')
fig.colorbar(im, ax=ax, label='Final A')

plt.tight_layout()
plt.savefig(out_dir / 'rel_sensitivity.png', bbox_inches='tight')
print('Saved: rel_sensitivity.png')
