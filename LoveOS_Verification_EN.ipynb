# -*- coding: utf-8 -*-
"""
Love-OS Business Negotiation Model (Information Thermodynamics)
- Success probability:
    P = (Trust^α * Clarity^β) / ((1 + ρ * CostResistance) * (1 + θ * EgoResistance))
- Dynamics: Time-step updates for Trust, Clarity, EgoResistance, CostResistance, Latency
- Policies: A (Honest & Clear), B (Gamey & Defensive), C (Progressive improvement)
"""

from dataclasses import dataclass
from typing import Dict, Callable
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# 1) Parameters & State
# -----------------------------
@dataclass
class Params:
    # Sensitivity (success probability)
    alpha: float = 1.2   # weight for Trust
    beta: float = 1.1    # weight for Clarity
    rho: float = 1.0     # sensitivity to CostResistance
    theta: float = 0.8   # sensitivity to EgoResistance

    # Update coefficients (dynamics)
    k_trust_up: float = 0.06      # honest acts increase Trust
    k_trust_down: float = 0.05    # dishonest acts decrease Trust
    k_clarity_up: float = 0.06    # clarification increases Clarity
    k_clarity_down: float = 0.04  # obfuscation decreases Clarity
    k_ego_stress: float = 0.05    # stress increases EgoResistance
    k_ego_align: float = 0.05     # alignment efforts reduce EgoResistance
    k_cost_reduce: float = 0.06   # guarantees/concessions reduce CostResistance
    k_cost_increase: float = 0.04 # new risks increase CostResistance

    noise_scale: float = 0.01     # environmental noise

@dataclass
class State:
    Trust: float          # psychological safety / perceived honesty
    Clarity: float        # proposal transparency / low ambiguity
    EgoResistance: float  # internal friction (defensiveness)
    CostResistance: float # price/risk/uncertainty friction
    Latency: float        # decision latency (for visualization)

def clip01(x: float) -> float:
    """Clamp to [0, 1]."""
    return max(0.0, min(1.0, x))

# -----------------------------
# 2) Success probability
# -----------------------------
def success_probability(st: State, p: Params) -> float:
    """
    P_success = (Trust^α * Clarity^β) / ((1 + ρ * CostResistance) * (1 + θ * EgoResistance))
    """
    numerator = (st.Trust ** p.alpha) * (st.Clarity ** p.beta)
    denominator = (1.0 + p.rho * st.CostResistance) * (1.0 + p.theta * st.EgoResistance)
    P = numerator / max(denominator, 1e-9)
    return clip01(P)

# -----------------------------
# 3) One-step dynamics
# -----------------------------
def step(st: State, actions: Dict[str, float], p: Params) -> State:
    """
    actions:
      'honesty'      : 0..1 (honest behavior)
      'dishonesty'   : 0..1 (dishonest behavior)
      'clarify'      : 0..1 (clarification effort)
      'obfuscate'    : 0..1 (ambiguity tactics)
      'stress'       : 0..1 (stress input)
      'align'        : 0..1 (alignment effort)
      'guarantee'    : 0..1 (guarantees/concessions to reduce cost)
      'risk_raise'   : 0..1 (new risk raising)
      'latency_push' : 0..1 (effort to act promptly)
    """
    n = p.noise_scale * (np.random.rand(4) - 0.5)  # noise for Trust, Clarity, Ego, Cost

    # Trust update
    st.Trust = clip01(st.Trust + p.k_trust_up * actions.get('honesty', 0.0)
                                - p.k_trust_down * actions.get('dishonesty', 0.0) + n[0])
    # Clarity update
    st.Clarity = clip01(st.Clarity + p.k_clarity_up * actions.get('clarify', 0.0)
                                  - p.k_clarity_down * actions.get('obfuscate', 0.0) + n[1])
    # EgoResistance update
    st.EgoResistance = clip01(st.EgoResistance + p.k_ego_stress * actions.get('stress', 0.0)
                                            - p.k_ego_align  * actions.get('align', 0.0) + n[2])
    # CostResistance update
    st.CostResistance = clip01(st.CostResistance - p.k_cost_reduce * actions.get('guarantee', 0.0)
                                               + p.k_cost_increase * actions.get('risk_raise', 0.0) + n[3])
    # Latency (for visualization): prompt effort reduces latency, higher ego raises it
    st.Latency = clip01(st.Latency - 0.05 * actions.get('latency_push', 0.0) + 0.04 * st.EgoResistance)

    return st

# -----------------------------
# 4) Strategy policies
# -----------------------------
def policy_A(t: int, st: State) -> Dict[str, float]:
    """Honest, clear, prompt; low stress; alignment + guarantees."""
    return {
        'honesty': 0.9, 'dishonesty': 0.1,
        'clarify': 0.8, 'obfuscate': 0.1,
        'stress': 0.2, 'align': 0.7,
        'guarantee': 0.6, 'risk_raise': 0.2,
        'latency_push': 0.8
    }

def policy_B(t: int, st: State) -> Dict[str, float]:
    """Gamey, ambiguous, defensive; high stress; weak guarantees; slow."""
    return {
        'honesty': 0.3, 'dishonesty': 0.6,
        'clarify': 0.3, 'obfuscate': 0.7,
        'stress': 0.7, 'align': 0.2,
        'guarantee': 0.2, 'risk_raise': 0.5,
        'latency_push': 0.2
    }

def policy_C(t: int, st: State) -> Dict[str, float]:
    """Progressive improvement over time (50 steps to near-best)."""
    prog = min(1.0, t / 50.0)
    return {
        'honesty': 0.4 + 0.5 * prog,
        'dishonesty': 0.6 - 0.5 * prog,
        'clarify': 0.4 + 0.4 * prog,
        'obfuscate': 0.6 - 0.4 * prog,
        'stress': 0.6 - 0.4 * prog,
        'align': 0.3 + 0.5 * prog,
        'guarantee': 0.2 + 0.6 * prog,
        'risk_raise': 0.5 - 0.3 * prog,
        'latency_push': 0.3 + 0.5 * prog
    }

# -----------------------------
# 5) Simulation
# -----------------------------
def simulate(T: int, init: State, p: Params, policy_fn: Callable[[int, State], Dict[str, float]]):
    traj = {k: [] for k in ['P', 'Trust', 'Clarity', 'EgoResistance', 'CostResistance', 'Latency']}
    st = State(**init.__dict__)  # copy
    for t in range(T):
        actions = policy_fn(t, st)
        st = step(st, actions, p)
        P_t = success_probability(st, p)
        for k, v in [
            ('P', P_t),
            ('Trust', st.Trust),
            ('Clarity', st.Clarity),
            ('EgoResistance', st.EgoResistance),
            ('CostResistance', st.CostResistance),
            ('Latency', st.Latency)
        ]:
            traj[k].append(v)
    return traj

# -----------------------------
# 6) Visualization & Sensitivity
# -----------------------------
def plot_time_series(traj_A, traj_B, traj_C, out_dir=None):
    fig, axes = plt.subplots(2, 3, figsize=(12, 6))
    # Success probability
    axes[0,0].plot(traj_A['P'], label='A Honest & Clear', color='C0')
    axes[0,0].plot(traj_B['P'], label='B Gamey & Defensive', color='C1')
    axes[0,0].plot(traj_C['P'], label='C Progressive', color='C2')
    axes[0,0].set_title('P_success(t)'); axes[0,0].legend(); axes[0,0].grid(True, alpha=0.3)

    # Trust / Clarity (C)
    axes[0,1].plot(traj_C['Trust'], label='Trust', color='C3')
    axes[0,1].plot(traj_C['Clarity'], label='Clarity', color='C4')
    axes[0,1].set_title('Trust / Clarity (C)'); axes[0,1].legend(); axes[0,1].grid(True, alpha=0.3)

    # Resistances (C)
    axes[0,2].plot(traj_C['EgoResistance'], label='Ego(R)', color='C5')
    axes[0,2].plot(traj_C['CostResistance'], label='Cost', color='C6')
    axes[0,2].set_title('Resistances (C)'); axes[0,2].legend(); axes[0,2].grid(True, alpha=0.3)

    # Latency (C)
    axes[1,0].plot(traj_C['Latency'], label='Latency', color='C7')
    axes[1,0].set_title('Latency (C)'); axes[1,0].legend(); axes[1,0].grid(True, alpha=0.3)

    # Trust A vs B
    axes[1,1].plot(traj_A['Trust'], label='A-Trust', color='C0', alpha=0.8)
    axes[1,1].plot(traj_B['Trust'], label='B-Trust', color='C1', alpha=0.8)
    axes[1,1].set_title('Trust (A vs B)'); axes[1,1].legend(); axes[1,1].grid(True, alpha=0.3)

    # Clarity A vs B
    axes[1,2].plot(traj_A['Clarity'], label='A-Clarity', color='C0')
    axes[1,2].plot(traj_B['Clarity'], label='B-Clarity', color='C1')
    axes[1,2].set_title('Clarity (A vs B)'); axes[1,2].legend(); axes[1,2].grid(True, alpha=0.3)

    plt.tight_layout()
    if out_dir:
        plt.savefig(f'{out_dir}/negotiation_dashboard.png', bbox_inches='tight')
        print('Saved:', f'{out_dir}/negotiation_dashboard.png')
    else:
        plt.show()

def sensitivity_heatmap(p: Params, init: State, policy_fn: Callable[[int, State], Dict[str, float]],
                        rho_range=(0.5, 2.0), theta_range=(0.4, 1.5), steps=30, T=100, out_dir=None):
    rhos = np.linspace(*rho_range, steps)
    thetas = np.linspace(*theta_range, steps)
    Z = np.zeros((steps, steps))
    for i, rho in enumerate(rhos):
        for j, theta in enumerate(thetas):
            p2 = Params(**{**p.__dict__, 'rho': rho, 'theta': theta})
            tr = simulate(T, init, p2, policy_fn)
            Z[i, j] = tr['P'][-1]  # final success probability

    fig, ax = plt.subplots(1, 1, figsize=(6, 4))
    im = ax.imshow(Z, origin='lower', aspect='auto', cmap='viridis',
                   extent=[thetas[0], thetas[-1], rhos[0], rhos[-1]])
    ax.set_xlabel('θ (EgoResistance sensitivity)'); ax.set_ylabel('ρ (CostResistance sensitivity)')
    ax.set_title('Final Success Probability (Sensitivity)')
    fig.colorbar(im, ax=ax, label='P_final')
    plt.tight_layout()
    if out_dir:
        plt.savefig(f'{out_dir}/negotiation_sensitivity.png', bbox_inches='tight')
        print('Saved:', f'{out_dir}/negotiation_sensitivity.png')
    else:
        plt.show()

# -----------------------------
# 7) Example run
# -----------------------------
if __name__ == "__main__":
    p = Params()
    init = State(Trust=0.5, Clarity=0.5, EgoResistance=0.6, CostResistance=0.6, Latency=0.5)
    T = 120

    # Simulate policies
    traj_A = simulate(T, init, p, policy_A)
    traj_B = simulate(T, init, p, policy_B)
    traj_C = simulate(T, init, p, policy_C)

    # Plot dashboard
    plot_time_series(traj_A, traj_B, traj_C, out_dir=None)

    # Sensitivity (with progressive policy C)
    sensitivity_heatmap(p, init, policy_C, steps=40, out_dir=None)
``
